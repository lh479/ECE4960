<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <meta name="description" content="">
        <meta name="author" content="">
        <title>ECE4960 Robby</title>
        <!-- Font Awesome icons (free version)-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js" crossorigin="anonymous"></script>
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet">
        <!-- Fonts CSS-->
        <link rel="stylesheet" href="css/heading.css">
        <link rel="stylesheet" href="css/body.css">
    </head>
    <body id="page-top">
        <nav class="navbar navbar-expand-lg bg-secondary fixed-top" id="mainNav">
            <div class="container"><a class="navbar-brand js-scroll-trigger" href="#page-top">FAST ROBOTS</a>
                <button class="navbar-toggler navbar-toggler-right font-weight-bold bg-primary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu <i class="fas fa-bars"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#portfolio">LABS</a>
                        </li>
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">ABOUT ME</a>
                        </li>
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#contact">CONTACT</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>
        <header class="masthead bg-primary text-white text-center">
            <div class="container d-flex align-items-center flex-column">
                <!-- Masthead Avatar Image--><img class="masthead-avatar mb-5" src="assets/img/PngItem_713104.png" alt="">
                <!-- Masthead Heading-->
                <h1 class="masthead-heading mb-0">ECE4960 | FAST ROBOTS</h1>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Masthead Subheading-->
                <p class="pre-wrap masthead-subheading font-weight-light mb-0">Robby Huang | ECE 2022</p>
            </div>
        </header>
        <section class="page-section portfolio" id="portfolio">
            <div class="container">
                <!-- Portfolio Section Heading-->
                <div class="text-center">
                    <h2 class="page-section-heading text-secondary mb-0 d-inline-block">LABS</h2>
                </div>
                <!-- Icon Divider-->
                <div class="divider-custom">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Portfolio Grid Items-->
                <div class="row justify-content-center">
                    <!-- Portfolio Items-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal0">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab1.png" alt="Lab1"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal1">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab2.png" alt="Lab2"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal2">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab3.png" alt="Lab3"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal3">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab4.png" alt="Lab4"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal4">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab5.png" alt="Lab5"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal5">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab6.png" alt="Lab6"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal6">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab7.png" alt="Lab7"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal7">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab8.png" alt="Lab8"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal8">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab9.png" alt="Lab9"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal9">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab10.png" alt="Lab10"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal10">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/Lab11.png" alt="Lab11"/>
                        </div>
                    </div>
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal11">
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white"><i class="fas fa-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/lab12.png" alt="Lab12"/>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- Portfolio Modal-->
        <div class="portfolio-modal modal fade" id="portfolioModal0" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal0Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 1: The Artemis Board</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    
                                    <!-- Portfolio Modal - Text-->
                                    <p align="left">The purpose of this lab is to help us set up IDE and run some simple scripts on the Artemis board. By running blink, serial communication, analog read of the temperature sensor, and frequency measurement by the Pulse Density Microphone, I was familiarized with the basic features of the board. </p>
                                    <iframe src = "https://www.youtube.com/embed/RLJkmN6ZvMs"
                                    width="540" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">In this first demo, I blinked the board. To successfully compile the script on my Mac I need to upgrade my Arduino IDE from 1.8.9 and decrease the SVL baud rate to <code>115200</code> bits per second. </p>
                                    <iframe src = "https://www.youtube.com/embed/Ba4FY0rEfFk"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">In the second example, I ran the example code on serial communication. Since we are only using the serial communication on the Type-C cable and the example code printed out messages on both UART channels, I modified the code so that everything printed out from the USB one. The <code>Serial.print()</code> function will allow you to send out information over UART. The video demonstrates both input and output with UART. </p>
                                    <iframe src = "https://www.youtube.com/embed/HXBCVLbETWs"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">In the third demo, I ran the analogRead example code and visualized the gradual temperature change on the temperature sensor caused by touching the chip on the serial monitor. Reading the analog value on an analog pin can be accomplished by calling the <code>analogRead()</code> function.</p>
                                    <iframe src = "https://www.youtube.com/embed/MVGxjuh36y8"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left"> The fourth demo showed the display of the highest frequency heard by the microphone on the board. After I ran the MicrophoneOutput example code, the highest frequency of the surrounding sound was displayed on the serial monitor. In the code, it first converts the PDM samples to floats in this line <p> <code>g_fPDMTimeDomain[2 * i] = pi16PDMData[i] / 1.0;</code> 
                                    <p align="left">and then perform FFT with the <code>arm_cfft_f32</code> function. Lastly, it finds the frequency bin with the largest magnitude and prints it on the serial monitor. As I whistled in higher and higher frequencies, the number displayed on the serial monitor increased. </p>
                                    <iframe src = "https://www.youtube.com/embed/q9I3fyrfy0I "
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">In the last demo, I slightly modified the microphoneOutput example. By using and <code>if/else</code> conditional statement with the variable <code>ui32MaxIndex</code>. 
                                    Code snippet that I added: </p>
                                    <img class="img-fluid" src="assets/img/Lab1Code.png">
                                    <p></p><p align="left">As you can see in the video, the on board led blinked when I whistled. 
                                    <p>

                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal1Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab2: Bluetooth</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    
                                    <!-- Portfolio Modal - Lab2-->
                                    <p align="left">The objective of this lab is to establish Bluetooth communication between the Artemis and the computer. We need to implement Arduino code on the Artemis side and Python in Jupyter Notebook on the computer. The purpose of this is to offload computation to the computer later when we are performing computationally intensive tasks on the robot.</p>
                                    <p class="pre-wrap lead mb-3">Setup</p>
                                    <p align="left">The first step was to set up the environment on the computer side. I initially tried to install Jupyter Lab on my Mac computer. However, since my Mac was still running MacOs 10, Jupyter Lab failed to open.</p>
                                    <img class="img-fluid" src="assets/img/lab2:1.png">
                                    <p align="left">I eventually switched to Ubuntu 20.02 and successfully ran Jupyter Server in my virtual environment. </p>
                                    <p align="left">The MCU setup was much easier. We just need to install the ArduinoBLE library and upload <code>ble_arduino.ino</code> to the Artemis board. If uploaded successfully, we should be able to see the MAC address of the board on Serial Monitor.</p>
                                    <p class="pre-wrap lead mb-3">Bluetooth Connection</p>
                                    <p align="left">To establish the connection between the Artemis board and the computer with BLE, I matched the MAC address defined in connection.yaml to the one on Arduino serial monitor. This message should be displayed when the connection is established.</p>
                                    <img class="img-fluid" src="assets/img/Lab2:2.png">
                                    <p class="pre-wrap lead mb-3">Demo</p>
                                    <p align="left">With the help of the demo code, I performed the following tasks: receiving float and string using <code>receive_float()</code> and <code>receive_string()</code> functions, sending commands, and disconnecting. Here is a short video running the tasks in the demo code.</p>
                                    <iframe src = "https://www.youtube.com/embed/j6YbJds9qWs"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3">Tasks</p>
                                    <p align="left">Before I demonstrate my tasks, I need to run the script to import necessary modules and run the connect command. </p>
                                    <iframe src = "https://www.youtube.com/embed/FGkSLJ77dJ4"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">TASK ONE: 
                                        The first task is to send an <code>ECHO</code> command with a string input that will trigger the Artemis board to return an augmented string to the computer. On the Artemis side, commands are handled by the switch/case statement. In the <code>ECHO</code> case, I first initialized a char array and then use the get value function to extract the string from the command string. After obtaining the input string, the next step is to augment the array and send it back to the computer. First I emptied the contents of the tx_estring_value, then use <code>append()</code> function to augment it. Lastly, I send the augmented string out with the writeValue commend. For debugging purposes, I also print it on the serial monitor. 
                                        </p>
                                        <img class="img-fluid" src="assets/img/ECHO.png">
                                    <p align="left">In my code, I augmented my message with “My boss said:”, so the board would repeat my message while referring me as his boss. In this demo video, I sent the commend using <code>send_commend(CMD.ECHO, “hi”)</code> and I got "My boss said: hi” from the board. </p>
                                    <iframe src = "https://www.youtube.com/embed/_06IpDADzYU"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">TASK TWO: 
                                        Sending three floats is very similar to the example <code>SEND_TWO_INTS</code>. I first declared three float variables and then used <code>get_next_value(float)</code> function to write the three floats to the variables. On the computer side, I sent three floats 1.2, 3.42, and -21.32 to cover test cases with different decimal places and different signs.                                         
                                        </p>
                                        <img class="img-fluid" src="assets/img/Send3F.png">
                                    <iframe src = "https://www.youtube.com/embed/vQMTR___Y1E"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">TASK THREE: 
                                        The notify mechanism in BLE will trigger the handler (or callback function) when data is changed. It is similar to an interrupt and an ISR. The only thing we need to do on the Arduino side is to define a UUID associated with a float. In this case, I am using the declared tx_characteristic_float, which increments 0.5 every half a second. 
                                        On the computer side, I called the <code>start_notify(uuid, handler)</code> function. It was quite confusing for me that the handler requires two parameters, yet the notify function only takes uuid and the handler itself as inputs. I later discovered that we could declare an input for the handler function outside the scope of the function. I used <code>read(uuid)</code> function to read the input byte array and store it in variable inputf as the second input of the handler function. The handler function then basically converted the byte array into float and store it in a global variable. The global variable will keep updating unless the function <code>stop_notify(uuid)</code> is called. 
                                        <img class="img-fluid" src="assets/img/notify.png">
                                        </p>
                                    <iframe src = "https://www.youtube.com/embed/FrYCEU2hdT4"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left"> Remember to disconnect. 
                                        </p>
                                    <iframe src = "https://www.youtube.com/embed/eQ1JXguyodQ"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">TASK FOUR: 
                                        The two approaches can all successfully deliver a float with the same accuracy. When sending numbers one by one, both approaches have identical speeds. The benefit of using <code>receive_float()</code> is that, it is more direct and does not require data type conversion. This approach is ideal for debugging but expensive computationally. If we send a large number of data quickly, it is more memory efficient and faster to combine sensor readings into a single <code>BLECStringCharactersitic</code> on the Arduino side and unpacked it when it reached the computer side. This would be useful when we need to stream, for example, accelerometer readings rapidly when the robot is moving fast. We need to be aware of the max message size (150 byte) when implementing this.                                        
                                        </p>


                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal2Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab3: Sensors</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Lab3 - Text-->
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">The purpose of this lab is to let us familiarize ourselves with TOF and IMU, the two important sensors that we will equip on the robot. We should first understand the sensor so that we can use it to collect useful data such as the distance of obstacles and pitch, roll, and yaw. Furthermore, we should learn the limit and constrain of our sensors such as sample rate, noise, range, and accuracy. 
                                    <p class="pre-wrap lead mb-3">Time-of-Flight Sensor </p>
                                    <p align="left">Wiring </p>
                                    <img class="img-fluid" src="assets/img/Lab3Wiring.jpg">
                                    <p align="left">1. I ran the I2C example script to scan I2C addresses for the sensors. When I turned both of them on, the serial window displayed all the I2C addresses. When I turned one of them off, I found address <code>0x29</code>. This is expected according to the Sparkfun page for this sensor.</p>
                                    <p align="left">2. The ToF sensor has three distance modes. Short distance mode is more immune to ambient light, but its maximum ranging distance is limited to 1.3m, compared to 4m of the long distance mode. Meanwhile, long distance mode allows the longest possible ranging distance but is easier to be impacted by ambient light. Different distance modes also affect the timing budget. In short distance mode, 20ms is the minimum timing budge and 140ms is the timing budget allows for the long distance mode. Increasing the timing budget increases the average power consumption and can improve the repeatability error. On the final robot, both modes would be helpful. We could use one mode for each TOF sensor and perform sensor fusion. Given the size of the robot, we are probably going to operate at short or short/mid range mode. If the map can extend up to 4 meters, we would have to use long distance mode even though the resolution may decrease. </p>
                                    <p align="left">3. I examined the sensor range, accuracy, repeatability, and ranging time. I also tried to compare sensor reading with different light conditions and surface textures. I used a 1.3-meter tape measure to measure the distance of a flat surface from the TOF sensor mounted vertically. All testing data below are based on the average of 100 sample points per measurement. </p>
                                    <img class="img-fluid" src="assets/img/Lab3:1.png">
                                    <p align="left">In short distance mode, the sensor has a range between 30mm to 1250 with less than 10% of error. The accuracy was consistently high until it reaches more than 800mm, at which the percentage error starts increasing linearly. </p>
                                    <img class="img-fluid" src="assets/img/Lab3:2.png">
                                    <p align="left">The standard deviation of 100 sample points at each distance can tell us how steady the measurement is at that distance. Similar to the accuracy curve, the consistency of measurement decreases beginning at the 800mm point exponentially. </p>
                                    <img class="img-fluid" src="assets/img/Lab3:3.png">
                                    <p align="left">In this chart, I compared the TOF collecting data in different environments and from different surfaces. We can see that less ambient light will make the data more accurate and consistent and different surface textures correspond to different accuracy because it deflects light a bit differently.  </p>
                                    <p align="left">The standard deviation of 100 sample points at each distance can tell us how steady the measurement is at that distance. Similar to the accuracy curve, the consistency of measurement decreases beginning at the 800mm point exponentially.  </p>
                                    <p align="left">I also try to print out the ranging time of the sensor. The time between when <code>startRanging()</code> is called to when <code>stopRanging()</code> is called does not change linearly with the obstacle distance. If the obstacle is in range, the ranging time is about 52144 microseconds and if the obstacle is out of range, the ranging time is 32064.  </p>
                                    <p align="left">I accomplished daisy chaining the two TOF sensors by manually toggling the XSHUT pins with GPIOs at appropriate times. I first turned all the sensors off by pulling their XSHUT pins LOW and then turning them on one at a time while setting the I2C address of the first one to prevent address conflict. To ensure this process is successful, I printed out the output of <code>distanceSensor1.getI2CAddress()</code> to see the new address and using this logic:
                                    <code>
                                        if (distanceSensor1.getI2CAddress() !=distanceSensor2.getI2CAddress()) {
                                            Serial.println("No conflicts!");
                                        }</code>
                                        to ensure that the addresses are not conflicting. 
                                         </p>
                                    <p align="left">Demo video for two TOF sensors working at the same time： </p>
                                    <iframe src = "https://www.youtube.com/embed/wthItQdLOUI"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3">Inertial Measurement Unit </p>
                                    <p align="left">After I wired the IMU to the Artemis board I scan the I2C channel to find the sensor’s address. I found <code>0x68</code>, which is expected because the datasheet indicated that the I2C address for this sensor is either <code>1101000</code> or <code>1101001</code> in binary depending on the AD0 value. To run Example1_Basics.ino, we need to change the AD0_VAL to 0 since from the datasheet we learned that address <code>0x68(1101000)</code> corresponds to <code>AD0 = 0</code>.</p>
                                    <p align="left">I am able to see both scaled and raw accelerometer and gyroscope data when running the example code. I plotted them out on the serial plotter so that I can visualize them. In the accelerator plot, we can see some noisy acceleration data in X, Y, and Z directions. Depending on the orientation of the sensor, some acceleration is always detected when the sensor is stationary due to gravity. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/Lab3Acc.png">
                                    <p align="left">The gyroscope measures the angular velocity. Its plot looks noisy and it is very sensitive to changes in velocity without any filtering. </p>
                                    <img class="img-fluid" src="assets/img/Lab3Gyro.png">
                                    <p align="left">Accelerator</p>
                                    <p align="left">Pitch and roll are calculated by these lines of code: </p>
                                    <p align="left"><code>pitch = 180 * atan2(myICM.accX(),myICM.accZ())/M_PI;</code></p>
                                    <p align="left"><code>roll  = 180 * atan2(myICM.accY(),myICM.accZ())/M_PI;</code></p>
                                    <p align="left">We cannot calculate the yaw value with accelerometer data because we can’t use gravity as a reference acceleration in the XY plane.</p>
                                    <p align="left">In this demo video, I am showing the output at {-90, 0, 90} degrees pitch and roll. The output is relatively accurate</p>
                                    <iframe src = "https://www.youtube.com/embed/MuK-JwMk45o"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">The next step is calibration. I measured pitch and roll to have the range of pitch[-89,89], Roll [-88,85], which means it has an accuracy of >95%. To calibrate it, I used the map function. For example, to map pitch to [-90,90]: <code>pitch = map(pitch, -89, 89, -88, 85);</code> </p>
                                    <p align="left">Gyroscope</p>
                                    <p align="left">In order to find the frequency of the unwanted noise in the output signal, we need to perform Fast Fourier Transform on the dataset. I set a timer in the code to make it stop collecting data after 10 seconds so that the data length is known and consistent. My dataset has a length of 288 and a sampling period of 3.3150e-2s. I exported the data and performed FFT in Matlab. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/Lab3Matlab.png">
                                    <p align="left">I performed FFT on a noisy dataset, which I collected while tapping the sensor, and a dataset with limited noise to distinguish the noise frequency buckets in the frequency spectrum. </p>
                                    <img class="img-fluid" src="assets/img/NoiseP.PNG">
                                    <img class="img-fluid" src="assets/img/NoiseR.PNG">
                                    <p align="left">I also did FFT on a regular dataset.</p>
                                    <img class="img-fluid" src="assets/img/NoNoiseP.PNG">
                                    <img class="img-fluid" src="assets/img/NoNoiseR.PNG"> 
                                    <p align="left">As you can see, the useful data are mostly at less than 1Hz. If fc = 1, RC = 0.159. Since alpha = T/(T+RC)  and T = 0.03315s, <code>alpha = 0.03315/(0.03315+0.159) = 0.1725</code>. With this alpha, I am able to achieve a plot with less noise:</p>
                                    <iframe src = "https://www.youtube.com/embed/Bz7A5F3gzOM"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">We can tell the alpha value is appropriate because the readings react to changes in a short enough time, so the time constant is not too big.</p>
                                    <p align="left">Gyroscope</p>
                                    <p align="left">Gyroscope provides us the angular velocity of the sensor. We can simply multiply the velocity with the dt between the sampling intervals to find the angular displacement. We can find dt with the help of a variable and the <code>micros()</code> function: </p>
                                    <p align="left"><code>dt = (micros() - last_time) /1000000;</code></p>
                                    <p align="left"><code>last_time = micros(); </code></p>
                                    <p align="left">and calculate roll, pitch and yaw with: </p>
                                    <p align="left"><code>roll_g = roll_g + myICM.gyrX() * dt;                                    </code></p>
                                    <p align="left"><code>pitch_g = pitch_g - myICM.gyrY() * dt;
                                    </code></p>
                                    <p align="left"><code>yaw_g = yaw_g + myICM.gyrZ() * dt;
                                    </code></p>
                                    <p align="left">The sign between the two terms could change depending on the sensor orientation.</p>
                                    <p align="left"><code></code></p>
                                    <p align="left">This serial plot is plotting the angle displacement output calculated from the accelerometer and gyroscope without any filter. One can see that the output calculated from the accelerometer is much noisier than the one from the gyroscope. </p>
                                    <img class="img-fluid" src="assets/img/lab3gryoacc.png">
                                    <p align="left">We can do a more direct comparison between the filtered data by accelerometer and gyroscope. </p>
                                    <iframe src = "https://www.youtube.com/embed/Ge62Un_VXSE"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">Although the data synchronized at the beginning of the demo, there are a few distinguishing differences between them. 1. Gyroscope data rely on the initial condition, but accelerometer does not. 2. Though the gyroscope is less noisy, its data are likely to drift, especially after some rapid changes in data. Meanwhile, accelerometer data will not drift since it uses gravity as the reference. 3. Data from the gyroscope can go above 90degree and the accelerometer’s data has a range between [-90,90]. </p>
                                    <p align="left">I tried decreasing the sampling frequency of the system. Obviously, data will be displayed at a slower rate. More importantly, the increase in sampling interval introduced more noise and inaccuracies in approximating the angular displacement. Thus, the data is more likely to drift. </p>

                                    <p align="left">Since both sensors have pros and cons, we can use a complementary filter to produce a final data output with less noise and high accuracy. Here, we need to double-check the sign of the myICM.gyrY() term. Since the frequency of potential noises is similar to the one we found, we could just use the same alpha value. </p>
                                    <p align="left"><code>pitch = (pitch - myICM.gyrY()*dt) * (1-alpha) + pitch_a * alpha;
                                    </code></p>
                                    <p align="left"><code>roll = (roll + myICM.gyrX()*dt) * (1-alpha) + roll_a * alpha;
                                    </code></p>
                                    <img class="img-fluid" src="assets/img/combineAccGyro.png">
                                    <p align="left">In this plot, the red line is data from the gyroscope, the blue curve is the low pass accelerator data, and the green one is from the output of the complementary filter. The red curve is drifted and the blue curve is a bit noisier than the green one.  </p>
                                    <iframe src = "https://www.youtube.com/embed/618lhaL_uTU"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">In this final demo video, I demonstrated the IMU getting accurate pitch and roll in the full range with relatively low noise. The yaw value (the green line), is only computed from the gyroscope data, so it is subject to drifting.   </p>
                                    <img class="img-fluid" src="assets/img/combineAccGyro.png">
                                    <p align="left">Similar to the TOF sensor’s different operation modes, we can select the DPS(degree per second) of the IMU. When we are performing tasks such as high-speed drifting, we should increase the DPS setting to at least 1000 to achieve accurate yaw of the robot. When we are performing PID turning in small increment angles, we should decrease DPS to 250 so that the sensor resolution is enhanced.  </p>
                                    <p class="pre-wrap lead mb-3">System-Level Wiring and Placement </p>
                                    <img class="img-fluid" src="assets/img/lab3p1.png">
                                    <p align="left">Since all the sensors can communicate in I2C, we simply daisy chain them all together and access to them with their unique addresses. We need to connect a GPIO from the Artemis to toggle the XSHUT pin on TOF2 since it has the same address as TOF1. Therefore, we need to follow the sequence described above to reset its address. I2C communication allows simply wiring while cascading on multiple slave devices. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab3p2.png">
                                    <p align="left">I decided to place the IMU in the front platform, one TOF in the front, and the other on the side. IMU needs to be placed parallel to the ground with a firm and stable platform. Therefore, I cut half of the battery case on the car so that it can use the plastic platform there. It is also an ideal location for the IMU since it is the furthest away from the motors. Motor EM waves add a lot of noise to the accelerometer readings. TOF1 and TOF2 are placed on vertical and open surfaces so that they can be mounted securely while providing more data points to localize the car in future labs. 
                                    </p>
                                    
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                    
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal3Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab4: Characterize Your Car</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Lab4 - Text-->
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">After learning about sensors in our previous lab, we learnt about the potential and limitations of our car in this lab. With the help of the remote control and IMU, my partner (Jack Defay) and I designed a series of tests to explore the ability of the RC car. 
                                    <p class="pre-wrap lead mb-3">Testing Method </p>
                                    <p align="left">While most of the tests relied on basic lab equipment such as rulers, scales, and stopwatches, we used Bluetooth to stream data from IMU during our test runs to obtain more interesting and informative data. </p>
                                    <p align="left">To establish such a connection, we used the <code>tx_estring_value.append(float value) </code>function and <code>tx_characteristic_string.writeValue(tx_estring_value.c_str()) </code>function to update the <code>BLE_TX_STRING</code> with <code>uuid “f235a225-6735-4d73-94cb-ee5dfce9ba83”</code>. On the Python side, we called the <code>start_notify</code> function to access that uuid and stream the changed data. We packaged all the float data into a string and update it at each cycle so the data is well-formatted and more memory efficient. Such connection enabled us to test the robot untethered while keeping all the sensor values well documented. </p>
                                    <p align="left">Since gyroscope reading tends to drift a lot while we are transferring the robot before testing, I made this <code>ZEROGYRO</code> commend so that we can zero the initial condition before each test to obtain cleaner test results. </p>
                                    <img class="img-fluid" src="assets/img/lab4:1.png">
                                    <p class="pre-wrap lead mb-3">Dimension </p>
                                    <p align="left">I used rulers and calipers to obtain accurate measurements for the car. Dimensions are important in robotics since it defines the local frame of the robot and directly affects the robot’s dynamics. Dimensions are shown in this picture in millimeters. </p>
                                    <img class="img-fluid" src="assets/img/Lab4:2.png">
                                    <p class="pre-wrap lead mb-3">Weight </p>
                                    <p align="left">I used a scale to measure the weight of the robot and the weights of its battery. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/IMG_5500.jpg">
                                    <img class="img-fluid" src="assets/img/IMG_5502.jpg">
                                    <p class="pre-wrap lead mb-3">Battery </p>
                                    <p align="left">To understand the battery and the power consumption of car better, I did an unloaded discharged test. I used the 650mAh 1S Lipo battery that comes with the car and run the car unloaded continuously. </p>
                                    <img class="img-fluid" src="assets/img/lab4:7.png">
                                    <p align="left">I obtained this discharge curve, which is a quite normal Lipo discharge curve. The car requires 3V minimal to run. A 3V battery typically corresponds to 10% SoC(state of charge). If we hang the car up with wheels off the ground, we get about 20 minutes of run time. I already tried to power the car with a DC power supply, but it doesn’t seem to be working due to the high ripple in the current drain.</p>
                                    <p class="pre-wrap lead mb-3">Turning Resolution </p>
                                    <p align="left">We wanted to figure out the turning resolution of the robot with remote control and IMU sensor so that we can better control the robot’s turning motion in the future. With gyroscope data (plotted below), we are able to see how much the robot turns when we press the button on the remote as short as possible.  </p>
                                    <img class="img-fluid" src="assets/img/Lab4:3.png">
                                    <p align="left">After using all the data and calculating the delta yaw at each turn, I found that the minimum turning with the remote control is about 40 degrees. </p>
                                    <img class="img-fluid" src="assets/img/lab4:4.png">
                                    <p class="pre-wrap lead mb-3">Ground Flipping </p>
                                    <p align="left">In this task, we would like to find out the minimum runup distance require to flip the RC car. To visualize the flipping motion, we plot the gyroscope data out during the run-up and flipping process.  </p>
                                    <img class="img-fluid" src="assets/img/lab4:5.png">
                                    <p align="left">As you can see, the robot performed a 360 flip here. The steps behind the flipping motion involve 1. The car to</p>
                                    <p align="left">The steps behind the flipping motion involve 1. The car ramps up to a certain speed. 2. The car breaks and the back of the car will be lifted up due to inertia. 3. The front wheel starts to spin in a reverse direction, while the back of the car keeps going to the front in the air. 4. Finish flipping. Given this process, the most important parameter would be the lowest initial speed. We characterized this by starting the car from a full stop and calculating the minimal distance to flip. We took videos to check the run-up distance at the frame that the car starts to flip and we obtained the following data. </p>
                                    <img class="img-fluid" src="assets/img/lab4:6.png">
                                    <p align="left">From the data from the 18 trials, 15 of them succeeded while 3 of them failed. We can see that 3 feet should be adequate for the robot to run up and flip.</p>
                                    <p align="left">Testing video:</p>
                                    <iframe src = "https://www.youtube.com/embed/WovTeWXbvu8"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3"> Ramp-Up Time</p>
                                    <p align="left">From observation, we concluded that the car ramps up to its highest speed in roughly 10 feet. We took multiple videos and calculated the time difference between the frame that the car started and the frame that the car crossed the 10 feet line. The data showed that the car took an average of 1.24 seconds to ramp up. The data has a small standard deviation of 1.108, so it is consistent.</p>
                                    <p align="left">Testing video:</p>
                                    <iframe src = "https://www.youtube.com/embed/umNwGOdhxLk"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3"> Bi-Pedal Motion</p>
                                    <p align="left"> During testing, we discovered this interesting motion of the car that happens when the car is sideway. When we oscillate between the two turning directions quickly, the car will achieve fish-like locomotion.</p>
                                    <iframe src = "https://www.youtube.com/embed/y_OgnBC79zs"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">With the help of the IMU, we are able to track the pitch of the car when performing such a motion. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/Lab4:8.png">
                                    <p align="left">Each movement can be further broken down into left commend, right commend, and left commend. We characterized this motion but counting the direction of the left and right spinning of the motor. We compared the good runs (red curves) and bad runs (blue curves) in the curves below and found the optimal range for motor spinning duraction to be around 0.3 seconds. Later when we are controlling the robot with PWM signals, we can ensure such duraction when it is doing the bipedal movement. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/Lab4:9.png">

                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal4Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Lab 5-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab5: Open Loop Control</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Text-->
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        In this lab, we start to run the car with our own control circuits and algorithm. With our own control system, we can control the exact speed and duration of each motor. This will set a solid foundation for closed-loop control and, more importantly, allow us to replicate the stunts we characterized later. 
                                        </p>
                                    <p class="pre-wrap lead mb-3">Wiring </p>
                                    <p align="left">
                                        Wiring is a crucial part of the lab. A clean, neat, and secure wiring will allow us to have a smooth testing process and obtain data without much noise. First of all, I took the car apart and cleaned the original circuitry out. There are a few requirements I hope to meet with the placement of all the components:
                                        </p>
                                    <p align="left">
                                        1. All components need to be securely mounted and in a position with enough space.
                                            </p>
                                    <p align="left">
                                         2. IMU should stay away from the DC motors so the accelerometer is less disrupted by the motor EMI noise.</p>
                                    <p align="left">
                                            3. Components with a lot of connections to each other should stay close together.</p>     
                                    <p align="left">Therefore, I decided to place the IMU in the front platform, one TOF in the front and one on the side, two motor drivers in the rear cabin with all the batteries, next to the motors and the Artemis. </p> 
                                    <p align="left">There are two h-briges on the <code>DRV8833</code> IC. To increase the maximum current we can deliver to each motor, we use one driver per motor by parallelling its two channels. This is a creative solution when we do not have a single h-bridge that can handle the amount of current we would like to deliver. </p> 
                                    <p align="left">To make debugging hardware easier, I decide to connect different parts of the system securely while making them detachable from each other. Therefore, half of my connection to the boards is soldered and the other half is male/female header connection. Knowing that all GPIO pins on Artemis Nano have PWM capability and saving space, I only put headers between A2-8 pins on the board. </p>
                                    <p align="left">Placement of components: </p>
                                    <img class="img-fluid" src="assets/img/lab5placement.png">
                                    <p align="left">I used soft wires to wire all the components together and color-coded them with standard conventions: red for 3V3, black for GND, etc. To reduce EMI between sensors and actuators, I twisted all wires with high-frequency signals together so that the overall branch has a net current of zero. I further protected the circuit by using zip ties to lessen the stress on the solder joints. I also cut the wires so that they could all fit on the chassis but not be too tight. I used electrical tape to isolate components from shorting each other during the runs and use Gorilla tape to securely mount the sensors on flat platforms on the robot. </p>
                                    <p align="left">Final layout:
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5wiring.jpg">
                                    <p class="pre-wrap lead mb-3">Motor Driver </p>
                                    <p align="left">When we power a system for the first time, it is usually a good idea to power it with a DC power supply. For this implementation, the power supply is acting as a 1S Lipo battery so I set it to 3.7V (the nominal voltage of a 1S Lipo battery) and turn the current limit to 3A since the motor driver allows up to 1.5A per channel. The battery we will end up using is a 25C 850mAh Lipo battery, which can output up to <code>25C* 0.85Ah = 21.25A</code>. Later we will use two different batteries. One power the Artemis and all the sensors and the other provides power for the DC motors. The first one will probably be drawing current at a milliampere level while the other would go up to more than 5 amps. Since the motor draws a lot of currents and its current draws changes rapidly, it would be a good idea to separate the high power and low power path and their power sources. 
                                    </p>
                                    <p align="left">I used PWM to control the output level of the motor driver following this chart:
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5chart.png">
                                    <p align="left">Since all GPIO pins on Artemis have PWM channels, we can simply call analogWrite(pin, level) function to output a PWM from 0-255. Here, I demonstrated my ability to control the PWM duty cycle on pin A2 with a loop that steps the PWM from 0 to 255.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5code1.png">
                                    <!-- scope-->  
                                    <iframe src = "https://www.youtube.com/embed/1uajJRABDew"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">To power one motor, we just need to set one pin connected to the input to LOW and the other to a PWM pulse. For example, <code>analogWrite(A3,0); analogWrite(A2,200); </code>
                                        In the following demo videos, I demonstrated that the wheels can spin in all directions with proper PWM input from the Artemis. I started with one wheel, then started to control both wheels.                                        
                                    </p>
                                     <!-- 1wheel-->  
                                     <iframe src = "https://www.youtube.com/embed/42XXbOofmow"
                                     width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                     <!-- 1wheel-->  
                                     <iframe src = "https://www.youtube.com/embed/lL5KUV5moDY"
                                     width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                     <!-- 2 wheels-->  
                                     <iframe src = "https://www.youtube.com/embed/aa7RbZNYHA8"
                                     width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">I then further characterized the input to these functions, so I know how the PWM will make the car move.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5code2.png">
                                    <p class="pre-wrap lead mb-3">Open Loop Control </p>
                                    <p align="left">This was what happened, when I ran the car on the floor with the battery plugged in. Although I added a timer to stop the movement after two seconds, it was still too moving too fast and not at a straight line. 
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/Smuuu7fAajw"
                                     width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">The functions I created in the previous sections were nice, but keep reprogramming the board is quite annoying. Therefore, I made a separate command for each one of them. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5code3.png">
                                    <p align="left">After connecting to Bluetooth with my computer, I can simply run commend like <code>ble.send_command(CMD.FORWARD, "")</code> 
                                    and <code>ble.send_command(CMD.CWTURN, "") </code>
                                        
                                        To make the car perform certain tasks.                                        
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5code4.png">
                                    <p align="left">This command will allow the user to input the PWM level of all the motors to set the speed of the car immediately with this line of code in Python.
                                       <code>ble.send_command(CMD.SETSPEED, "255") </code> 
                                        Since it is moving to the left when I commanded it to go straight, I tried adding some calibration factor on the left motor to make it go a bit faster than the right one and found that a factor of 1.1 works the best. <code>analogWrite(A2,motorspeed*1.1);</code>
                                        In this demo, the remote control car moves fairly straight. The string under the car is 2 meters long.                   
                                    </p>


                                    <!-- straight demo-->  
                                    <iframe src = "https://www.youtube.com/embed/ANKu8Zj6XOM"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>

                                    <p align="left">With the SETSPEED commend, we can also easily find the minimal PWM duty cycle to make the car move. When the battery is at 4V, the car moves when the PWM level is 49 (which means a duty cycle of <code>49/255 = 19.2%</code>). This video shows the difference between the car moving forward at 19.2% duty cycle and 18.2% duty cycle. When I ran it with a speed of 48, the car could no longer move forward smoothly.</p>
                                    <iframe src = "https://www.youtube.com/embed/WEBsITOBh4k"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>

                                    
                                    <p align="left">For the final demo, I showed the car moving with these command sequences over Bluetooth. Turning requires the motor to spin harder so I set the speed to 255 before calling the turning commands. Overall, the robot could received and performed the commands accurately.      
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab5com.png">
                                    <!-- final demo-->    
                                    <iframe src = "https://www.youtube.com/embed/QKOxmwcW91o"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal5Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 6: Closed-loop control </h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    
                                    <!-- Portfolio Modal - Text-->
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        After we performed open-loop control in the previous lab, we are moving forward to closed-loop control. Closed-loop control, especially PID control, is a simple and elegant way to accurately control the robot. This lab allows us to set up the basis for impressive stunts later. We can practice implementing PID control from scratch and implementing it on the physical robot. Doing so is more challenging than tuning PID with equations in a simulator, but it encourages thinking about elements such as noise and sampling speed in a real-life scenario. I choose to implement task B for this lab. 
                                        </p>
                                    <p class="pre-wrap lead mb-3">Debugging Framework </p>
                                    <p align="left">
                                        First and foremost, we need to set up a user-friendly debugging environment so that we can implement a PID controller more efficiently later. I first allocated memory for a few 500-element arrays. Based on the sample speed of the IMU, 500 data points will take about 5 seconds to be filled, which should be long enough to perform the task. I created the following arrays for debugging or tuning PID. 
                                        </p>
                                    <p align="left">
                                        To send them over in a clean and speedy manner, I decided to make a <code>SENDDATA</code> commend, in which I made a for loop with 500 interactions. In each iteration, I cleared the <code>tx_estring_value</code> and appended all the debugging data on it. Therefore, all the data can be associated with the correct timestamp. 
                                            </p>
                                            <img class="img-fluid" src="assets/img/Lab6:1.png">
                                    <p align="left">
                                        On the Python side, I declared a list “readings”  to store all the strings. I called the start_notify function to monitor any changes in the <code>RX_STRING</code>. In the handler, I appended the transferred strings to the list. Once the handler was established, I called the <code>SENDDATA</code> commend. Usually, after about 20 seconds, all the data are transferred and I used a for loop to print out the readings: </p>
                                        <p align="left"><code>for f in readings:</code></p>
                                        <p align="left"><code>print(f)</code> </p>
                                        <p align="left">The readings are printed out in one row per timestamp format with commas separating different data. I can copy the data and plot them in Google Sheet.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab6:2.png">
                                    <p align="left">
                                        To reduce the number of compilations I need to do, I created two helper commends <code>SETPIDVALUE</code> and <code>SETSETPOINT</code>. I call them in the Python code so I can tune my PID values and change the setpoint quickly. 
                                        <code>ble.send_command(CMD.SETPIDVALUE, "4|0|0.5")</code>
                                        <code>ble.send_command(CMD.SETSETPOINT, "0")</code>
                                    </p>
                                    <p align="left">
                                        On the Arduino side, I used the function from the ble library to update global variables such as Kp, Ki, Kd, and setPoint. For example, to update the Kp value from user input, I used <code>success = robot_cmd.get_next_value(Kp);</code></p>
                                        <p align="left">Since I used Bluetooth to send commends in lab4 and lab5, I was able to reuse a lot of my functions such as <code>forward(), backward(), CW(), CCW(),</code> and <code>brake().</code> I also demonstrated my deployment of the IMU functionality onto the Bluetooth code in the Testing Methods section of my lab4 writeup.</p>
                                        <p align="left">After implementing all the debugging and helper functions, I am ready to move forward to implement PID control.</p>
                                    <p class="pre-wrap lead mb-3">PID Logic </p>
                                    <p align="left">A PID controller continuously calculates an error value as the difference between the desired setpoint and a measured process variable (output), and it applies a correction based on proportional (P), integral (I), and derivative (D) terms. The objective of this PID controller is to make the robot turn facing a certain direction. The key to implementing a PID controller is knowing what the setpoint, input, output, and error are. More importantly, knowing their units. In this case, the setpoint is the target angle, input is the yaw in degree calculated from the gyroscope z-axis data, the error is the difference between the current yaw and the setpoint, and the output is a number generated by the three PID terms. The P term is the product between a constant Kp and the error. The I term is the product of Ki and accumulated error. The D term is the product between Kd and the change of error. The unit of the output does not really matter. In this case, I use the sign of the output to decide if I should make the robot turn clockwise or counterclockwise, and the magnitude to control the motor speed. To make sure motor speed is in a reasonable range, I used a combination of min and max function to ensure its value. <code>motorspeed = min (230, max(150, output)); </code>
                                    </p>
                                    <p class="pre-wrap lead mb-3">PID Implementation and Tuning </p>
                                    <p align="left">Before tuning the controller, let me briefly summarize the significance of Kp, Ki, and Kd. The bigger Kp is, the harder the controller pushes. The smaller Ki is, the quicker the controller reacts to load changes, but the greater risk of oscillations. The bigger Kd is, the more the controller dampens oscillations. 
                                    </p>
                                    <p align="left">I implemented a controller with only the proportional term first. The value of Kp is arbitrary, but we can approximate the order of magnitude by considering that our output will become the input of analogWrite (0-255) and our error is (0-180) if we want the robot to turn backward. Therefore, I started with Kp equaled 1. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/Lab6C1.png">
                                    <p align="left">I plotted the angle vs time, and I could see that system reached the setpoint correctly but it was a bit slow. Therefore, I kept increasing the Kp value. I observed overshoot when I set it to 3. </p>
                                    <img class="img-fluid" src="assets/img/Lab6C2.png">
                                    <p align="left">To suppress the overshoot while increasing the speed, even more, I increased the Kp to 4 and introduced a differential term. A big differential term slowed down the system, so I kept it small. As you can see, the overshoot is suppressed. </p>

                                    <img class="img-fluid" src="assets/img/Lab6C3.png">
                                    <p align="left">Since the robot turned fast and accurately with PD controls, my final implementation only include those two terms. 
                                    </p><img class="img-fluid" src="assets/img/Lab6:4.png">
                                    <p align="left">A worth noting debugging detail is that my gyroscope data was inaccurate at first. I then realized that the raw gyroscope data were maxed out. The gyroscope has an advanced setting that we can address, the maximal speed of the sensor. I increased in from the default 250 degree per second to 1000 with myFSS.g = dps1000;                                    </p>
                                    <p class="pre-wrap lead mb-3">Drifting Implementation </p>
                                    <p align="left">With effective closed-loop turning, all we need to do right now is to modify it so that it can move forward before and after. I set the initial setpoint to be 0, and change the setpoint in the middle of the run. I then ensured the robot moved forward when it is facing the setpoint direction by adding the if statement: <code>if (abs(error) < 2) { forward();}</code>
                                    And adding the pid turning code in the else statement.
                                    </p>
                                    <p class="pre-wrap lead mb-3">Demo</p>
                                    <p align="left">Here is the demo video for drifting. The robot traveled straight, turned exactly 180 degrees, and then traveled straight back. It performed the entire task at a relatively high speed.</p>
                                    <iframe src = "https://www.youtube.com/embed/o0zs07wxH40"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">Below are the data collected during this demo run.</p>
                                    <img class="img-fluid" src="assets/img/lab6C4.png">
                                    <img class="img-fluid" src="assets/img/lab6c5.png">
                                    <img class="img-fluid" src="assets/img/lab6c6.png">
                                    <iframe src = "https://www.youtube.com/embed/Z8X0P8SSXpY"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <iframe src = "https://www.youtube.com/embed/OfjwZr5vnvs"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">Below are the data collected during this demo run.</p>
                                    <img class="img-fluid" src="assets/img/lab6c7.png">
                                    <img class="img-fluid" src="assets/img/lab6c8.png">
                                    <img class="img-fluid" src="assets/img/lab6c9.png">
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal6Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 7: Kalman Filter</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        Kalman filtering is an algorithm that incorporates uncertainty to obtain a better estimate based on inputs and observations. The purpose of this lab is to implement a Kalman filter so that we can quickly predict the distance of the robot from the wall. In task B, the robot needs to turn quickly when it gets too close to the wall. With the ToF sensor, we are able to measure the distance of the robot to the wall, yet the sensor is too slow for the fast robot. Therefore, we need to employ a Kalman filter to estimate the distance to the wall frequently.
                                    </p>
                                    
                                    <p class="pre-wrap lead mb-3">Step Response </p>
                                    <p align="left">
                                            In order to obtain the state space equation of the robot, we need to first execute a step response. Similar to the previous lab, I implemented commends so that I can control the robot over Bluetooth. 
                                            </p>    
                                    <img class="img-fluid" src="assets/img/Lab7:1.png">
                                    <p align="left">
                                        In the <code>STEPRESPONSE</code> command, I drove the robot against the wall at maximal speed and made it hard-break when it gets too close. Since I am driving the robot at a very high speed, I account for the braking distance before the robot hits the wall. I stored the testing data in the TOFdata array and send it over after the run using Bluetooth.
                                        </p> 
                                        <img class="img-fluid" src="assets/img/lab7:2.png">
                                        <img class="img-fluid" src="assets/img/lab7:3.png">
                                        <p align="left">
                                            I plotted the data collected during the run. In order to find the steady-state speed and the 90% rise time, I plotted the speed versus time curve, which is the derivative of distance versus time curve in Matlab. 
                                        </p> 
                                     <img class="img-fluid" src="assets/img/lab7:4.png">
                                     <p align="left">
                                        <code>F = ma = mx”
                                        </code>
                                    </p> 
                                    <p align="left">
                                        <code>F = u - dx’
                                        </code>
                                    </p> 
                                    <p align="left">
                                        <code>x” = u/m - dx’/m
                                        </code>
                                    </p> 
                                    <p align="left">
                                        The space equations are:
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7eq.png">
                                    <img class="img-fluid" src="assets/img/lab7eq2.png">
                                    <p align="left">
                                        To find drag(d), we use the steady-state speed. From the plots, the speed is about 3200. 
                                            <code>d = u/ x’ = 1/3200</code>
                                    </p> 
                                    <p align="left">
                                        To find the momentum(m), we use the 90% rise time of the speed. 
                                            <code>M = - d * t_r / np.log(0.1)
                                            </code>
                                    </p> 
                                    <img class="img-fluid" src="assets/img/Lab7code1.png">
                                    <p align="left">
                                        Then I used d and m terms to find the A and B matrices.I also found delta T by averaging the sampling times. The next step is to discretize the A and B matrics. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7code2.png">
                                    <p align="left">
                                        Code snippets: 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7code3.png">
                                    <p align="left">
                                        Here, I am using the process noise and sensor noise covariances as the lecture slide suggested. Their values determine the behavior of the Kalman filter. The Kalman Filter will give more trust to the measurement when the process noise is large. On the other hand, when the sensor noise is large, the KF has more confidence in the prediction. I will tune the noise covariance matrices as I run the KF with data.
                                    </p> 
                                    <p align="left">
                                        I used the code from the lab handout for KF implementation. There are two stages in the Kalman Filter implementation: prediction and update. Prediction calculates mu_bar and signma_bar. In the update stage, we calculate the Kalman gain with sigma_bar and the noise. Then, use that to update the mu and sigma. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7code4.png">
                                    <p class="pre-wrap lead mb-3">Kalman Filter Sanity Check </p>
                                    <p align="left">
                                        Before implementing the KF on the Artemis, we need to run the KF on the previous data we collected to perform a sanity check. The initial value is <code>[Initial TOF val 0]</code>. To iterate all the datapoint, I looped through all timestamps and call the KF function at each iteration.
                                    </p> 
                                    <p align="left">
                                        By changing different noise values, the predicted values conform with measured values differently. When sigma_u is large the uncertainty of the model is high. This means we trust the sensor much more than our model and the curve, as you can see, follows the sensor values including the noisy part closely.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7g1.png">
                                    <p align="left">
                                        When I decreased sigma_u and increased the sensor noise, sigma_z, the predicted curve still follows the measured points in a much more lenient way. It is trusting the model more now. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab7g2.png">
                                    <p class="pre-wrap lead mb-3">Kalman Filter on Robot</p>
                                    <p align="left">
                                        Implementing what we achieved in Python is merely a process of rewriting the program in a different language and then integrating it into the original code.

                                    </p>
                                    <p align="left">
                                        The first step is to include the library we need, so we can perform matrices computations in Arduino IDE.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab7code11.png">
                                    <p align="left">
                                        Then we declare the calculated d and m constants and use them to declare the A, B, and C matrices. We also declare the process and measurement noise contained in sig_u and sig_z matrices.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab7code12.png">
                                    <p align="left">
                                        Similar to the Python code, I discretize matrices A and B.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab7code13.png">
                                    <p align="left">
                                        Lastly, I declare the initial states. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab7code14.png">
                                    <p align="left">
                                        We need need to call this function when we obtained new data from the TOF sensor and use the <code>x_val</code> calculated from the model instead of the sensor reading as the real distance of the robot to the wall. After a few small tunings and debugging of the car, the robot can stop more rapidly compared to before using KF.
 
                                    </p>
                                    <p align="left">
                                       Demo Video:
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/oWBN20DKT1c"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        As you can see, the car now can stop before it craches into the wall.
                                     </p>
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal7Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-7">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab8: Stunts</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        This lab is a summary and demo of the previous two labs. It is the moment when we make things that work separately to work together and perform the cool stunts. A lot of components can contribute to a successful final demo, including timing, floor and wheel condition, and code efficiency. We are also encouraged to use our imagination and what we have tried for fun in the previous lab to perform an open-loop stunt.
                                    </p>
                                    <p class="pre-wrap lead mb-3">Controlled Stunts: Task B: Drift Much?
                                    </p>
                                    <p align="left">
                                        Since I selected task B in the previous lab, my robot needs to start from more than 4 meters away, drive straight, cross a line 0.6m from the wall, drift for 180 degrees, and then run back fast. Since I already implemented drifting and TOF distance sensing from the wall with Kalman Filter in the previous two labs, I just need to combine the two parts. In Lab 6, my robot drifts 180 degrees when the data string is half full. Now, I just need to change that condition to be: when the robot is closer than 0.6m to the wall, we change the setpoint from 0 to 180 and the robot will spin to position itself to the correct angular position. When the error between the current pose and the setpoint is too small, the robot will just move straight as before. For the detailed implementation of the PID or KF, see previous labs. 
                                            </p>  
                                    <p align="left">
                                        Here is a full demo video:
                                    </p> 
                                    <iframe src = "https://www.youtube.com/embed/TDBU-BpFCZk"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3">Open Loop, Repeatable Stunts
                                    </p>
                                    <p align="left">
                                        I repeated the stunt I performed in Lab5: Bi-Pedal Motion. This interesting motion of the car happens when the car is sideway. When we oscillate between the two turning directions quickly, the car will achieve fish-like locomotion. Based on the plots I plotted for lab5 (see below), when the interval is larger than 3.5 seconds, or 350 milliseconds, the car can reliably perform the locomotion. 
                                    </p>  
                                    <img class="img-fluid" src="assets/img/Lab4:9.png">
                                    <p align="left">
                                        Therefore, in the code, I simply oscillated between the forward and backward function with a 350ms interval.
                                            </p>  
                                    <img class="img-fluid" src="assets/img/lab8:code.png">
                                    <p align="left">
                                        This is a demo video showing that the robot performs the bi-pedal motion reliable for 3 times consecutively.
                                    </p>  
                                    <iframe src = "https://www.youtube.com/embed/z_bCWaSYEqg"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3"> Blooper Videos
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/vHmbPMQMeo4"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal8Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab9: Mapping (real)</h2>
                                    
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        Mapping is a very important skill in robotics. For example, a smart vacuum robot usually first acquires the map of the entire room before motion planning. In this lab, we are employing our robot and the TOF sensor on it to create a map for the static map in the front room of the lab. To build the map, we can need to first make the robot spin in place while collecting data and then, with the help of the transformation matrix, convert the data collected in the robotics frame into the global frame.

                                    </p>  
                                    <p class="pre-wrap lead mb-3">Spinning In Place </p>
                                    <p align="left">
                                        Since I already completed task B in lab 6, I am using a PD controller to closed-loop control the robot to spin exactly a small degree every time it turns. The implementation of the controller is covered in lab6: 

                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab9:1.png">
                                    <p align="left">
                                        The accuracy of the system is already tested in the previous lab. Under a high-speed camera, you can see that the robot can turn accurately 180 degrees, as I instructed it to.  
                                    </p>  
                                    <iframe src = "https://www.youtube.com/embed/o0zs07wxH40"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        Here is a graph demonstrating the robot’s pose when the setpoint is 180 degrees.
                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab9:2.png">
                                    <p align="left">
                                        Since in this lab, the robot is turning slower, I decreased the degree per second setting to increase the accuracy of the system. In order to collect enough data for each turn, I am making the robot turn 10 degrees steps and make 36 steps every time the function is called. Therefore, I can visually inspect the quality of the run. The reason why I choose this step size is that the robot is able to fully ramp up its speed to make an accurate turn. Here is a video of it turning on-axis in the lab. 
                                    </p>  
                                    <p align="left">
                                        Since in this lab, the robot is turning slower, I decreased the degree per second setting to increase the accuracy of the system. In order to collect enough data for each turn, I am making the robot turn 10 degrees steps and make 36 steps every time the function is called. Therefore, I can visually inspect the quality of the run. The reason why I choose this step size is that the robot is able to fully ramp up its speed to make an accurate turn. Here is a video of it turning on-axis in the lab. 
                                    </p>  
                                    <iframe src = "https://www.youtube.com/embed/MGHJvnm0b5Y"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        Since in this lab, the robot is turning slower, I decreased the degree per second setting to increase the accuracy of the system. In order to collect enough data for each turn, I am making the robot turn 10 degrees steps and make 36 steps every time the function is called. Therefore, I can visually inspect the quality of the run. The reason why I choose this step size is that the robot is able to fully ramp up its speed to make an accurate turn. Here is a video of it turning on-axis in the lab. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9:3.png">
                                    <p align="left">
                                        Using my method of turning, the robot can plot out the orange dots as the map after changing the coordinates of its data points in the middle of a 4x4m square empty room. With a single turn, the resolution is not very high, but one can still have a preliminary understanding of the space. By inspecting if the robot stopped at its starting point, we can ensure the quality of the turn and thus eliminate the majority of the noise in yaw reading. Therefore, the noise of the mapping data is the TOF sensor reading noise, which can be estimated as a gaussian distribution with a mean of 0. 

                                    </p> 
                                    <p class="pre-wrap lead mb-3">Readout Distances</p>
                                    <p align="left">
                                        The reading distance process is the same as in the previous labs. We start ranging, wait until data is available, and store the read value into a variable, then stop ranging.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9:5.png">
                                    <p class="pre-wrap lead mb-3">Mapping Function</p>
                                    <p align="left">
                                        I implemented all mapping-related steps into the mapping commend. It takes the user input from the Bluetooth command for the number of data the robot collects. 

                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9:4.png">
                                    <p align="left">
                                        And then, I declared all the variables. Then, I am calculating the KP and KD values after a new yaw value is ready. When the error is small enough (less than 0.2 degrees),  the robot brakes, toggles the led on Artemis, and collects a TOF reading. When a reading is obtained, I increment the setpoint by 10 degrees until enough data point is collected. If the error is not small enough, the robot will turn to try to reach the setpoint. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9:6.png">
                                    <p align="left">
                                        The command on the Python side is <code>ble.send_command(CMD.MAP, “36”)</code>. The data are sent back the same way as before, in strings.
                                    </p> 
                                    <p align="left">
                                        After I called the command to run the robot on the map, I sanity-checked each of my runs by plotting the data collected in polar plots in Matlab. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9:p1.png">
                                    <img class="img-fluid" src="assets/img/lab9:p2.png">
                                    <p align="left">
                                        For each point in the map, I collected data with the initial pose of 0 degrees and -45 degrees. This not only provides a more unique data point for that specific location but also helps us to check if the two runs provide us with a similar map. As you can see, for the (0,3) point, the two sanity checkpoint are similar to each other and with a 45-degree difference as expected.
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Merge and Plot your Readings</p>

                                    <p align="left">
                                        I first implemented this robot2global function to perform the change of coordinate in Matlab. It takes in the robot’s current pose, a point in the robot coordinate, and then outputs the x,y coordinates of that point in the global frame. It basically uses the transformation matrix to perform the conversion.

                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9f1.png">
                                    <img class="img-fluid" src="assets/img/lab9c1.png">
                                    <p align="left">
                                        For each data, I use the function I implemented to calculate its coordinates in the global frame. This diagram will illustrate the conversion process. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9dia.png">
                                    <p align="left">
                                        Here are the implementations for points at (0,3) with an initial pose of 0 degrees to y-axis ( 90’ from x) and an initial pose of -45 degrees to y-axis (135’ from x).
                                    </p> 
                                    <p align="left">
                                        I collected 36 data points for each initial pose at each data collection point. Therefore, I collected a total of 8 times 36 = 288 data points. After I converted all the data points to the global frame, one can clearly see the contour of the map.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9c2.png">
                                    <p align="left">
                                        Here are the implementations for points at (0,3) with an initial pose of 0 degrees to y-axis ( 90’ from x) and an initial pose of -45 degrees to y-axis (135’ from x).
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9m1.png">
                                    <p align="left">
                                        I collected 36 data points for each initial pose at each data collection point. Therefore, I collected a total of 8 times 36 = 288 data points. After I converted all the data points to the global frame, one can clearly see the contour of the map.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab9m2.png">
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="portfolio-modal modal fade" id="portfolioModal9" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal9Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 10: Simulator</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    
                                    <p class="pre-wrap lead mb-3">Objective</p>
                                    <p align="left">
                                        This lab is getting us ready for the future lab by instructing us to set up the simulation environment, have fun simple tasks to guide us to engage with the virtual robot, and employ the live plotting tool.
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Setup</p>
                                    <p align="left">
                                        First, we need to update our Python to Python 3.10 and pip to at least 21.0. Pip did not work in the virtual environment so I decided to download Box2D and wheels to my own directory. I also need to deal with a bunch of dependencies issues through a lot of Google searchings. In the end, I successfully installed it from source. With the lab10 code base, I am able to see the simulation environment and maneuver the virtual robot with my keyboard. 
                                    </p>
                                    <p class="pre-wrap lead mb-3">Simulator and Its Functionality
                                    </p>
                                    <p align="left">
                                        The simulator is a great tool for us to preimplement algorithms before directly putting it on the robot. The real world has so many sources of noise that it would be challenging to implement algorithms without experimenting in an ideal environment. There are two options to start, stop, or restart the simulator and the plotter. The first one it to use the GUI. After calling “<code>gui.show()</code>”, the GUI will show up. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p1.png">
                                    <p align="left">
                                        The second option is to start the simulator and plotter programmatically, and we can reset and quit them in a similar manner.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p2.png">
                                    <p align="left">
                                        To interact with the GUI, we can use our mouse and keyboard. Pressing ‘h’ will show this complete list of commands for controlling the GUI. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p3.png">
                                    <p align="left">
                                        The plotter is more direct. We can simply use our mouse to zoom in and out, and press the “A” button to autofit the plot to our window. 
 
                                    </p>
                                    <p align="left">
                                        We can also interact with the simulator and the plotter through program to control the robot more precisely and plot specific information on the plotter. The commander class has a list of useful functions that we can call. For example, “<code>cmdr.plot_odom(x,y)</code>” will plot a point in the simulator in red. We use this function to plot odometry data. 
                                    </p>
                                    <p class="pre-wrap lead mb-3">Open Loop Control</p>
                                    <p align="left">
                                        To make the robot go in a square open loop, we just need to make the robot to go straight for a certain distance, turn 90 degrees, then go straight for the same distance, and repeat this sequence. I implemented three helper functions. First, a plotting function that plot the ground truth and odometry pose. Then, the <code>straight()</code> funciton that set velocity of the robot and call a non-blocking delay for 1 second: <code>await.asyncio.sleep(1)</code>. The <code>turn()</code> function is the hardest to implement open loop. Without feedback, I can only keep trying until I get the perfect combo for turning 90 degrees: an angular speed of 1 with 1.5 second duration. 
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p5.png">
                                    <p align="left">
                                        The next step is then run all three functions together in a loop.
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p6.png">
                                    <p align="left">
                                        The open loop simulation is shown in the video. As you can see, the odometry is not a reliable indication for the location of the robot. Therefore, we will later use Bayes filter to perform localization. Since there is no feedback, any noise or error will accumulate. You can see the robot does not run through the same path every time. The execution is not the same shape. 
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/yacUo6LzZHE"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3">Open Loop Control</p>
                                    <p align="left">
                                        Closed loop control means controlling the robot with the help of sensor input. I made the robot turn about 100 degrees whenever the sensor reads less than 0.5 meter. Otherwise, it will travel straight. There are three factors that determine if the robot can successfully detect the wall in front of it: the threshold turning distance, the velocity of the robot, and the duration of each straight function. If we want the robot to always turn successfully we would want it to travel slowly with a larger turning threshold and make each function shorter to increase sample rate. To make the comparison more direct and clear, I will keep the start turning distance at 0.5m. 
                                    </p>
                                    <p align="left">
                                        In this video, the robot is running straight at 0.5m/s speed and the <code>straight()</code> function has a 1 second delay in it. As you can see, the robot can barely prevent itself from crashing into walls. 
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/ACK1OfVGO7Y"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        In the next video, I slowed down the robot to 0.3m/s. It can stop and turn much more accurately than before. However, this system runs very slowly. We need to speed it up since this course is called Fast Robots. 
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/drDoeupKwGE"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        In this video, I increased the speed to 10 times of before, 3m/s. At the same time, I shorten the duration of the function to travel straight from 1 second to 0.001 second. Even though the robot runs very fast, but it always stops itself before crashing into the wall. 
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/qmR4gH0mpAE"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        When the sample rate of the sensor is high enough — the time frame between each sensor reading is checked times the speed need to be small then the breaking distance — the robot avoids hitting the wall reliably. Except for one case: when there is a path like this one
                                    </p>
                                    <img class="img-fluid" src="assets/img/lab10p7.png">
                                    <p align="left">
                                        Since the distance sensor does not account for the width of the robot, when the robot moves straight, it will crash into the corner of the wall. A way to avoid this is to have the robot move carefully and turn at small angles on both sides after moving a short distance. 
                                    </p>
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal10" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal10Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 11: Localization (sim)</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        The purpose of this lab is to implement grid localization using the Bayes filter. Bayes filter is a general probabilistic approach for estimating an unknown probability density function recursively over time using incoming measurements and a mathematical process model. In our case, with odometry data and distance sensor data, we are able to approximate the localization of the robot.

                                    </p>  
                                    <p class="pre-wrap lead mb-3">Overview</p>
                                    <p align="left">
                                        The input of the Bayes Filter Algorithm is an array of belief, u_t, and z_t. The array of beliefs, in the context of this lab, is a 3-D array that stores the probability of the robot pose at a certain instance of X, Y, and angle. U_t is the control of the robot and Z_t is the array of the sensor values. There are two steps of a Bayes filter algorithm, the prediction step, and the update step. The later sections will focus on their detailed implementation. At the end of the algorithm, one should achieve a better estimate of the pose and location of the robot.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g1.png">
                                    <p class="pre-wrap lead mb-3">Compute_Control</p>
                                    <p align="left">
                                        First, I implemented the computer_control helper function. This function extracts the control information based on the odometry motion model with the current and previous odometry poses. In short, I used the three equations covered in the slides to implement this function.
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g2.png">
                                    <p align="left">
                                        The python implementation is:
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g3.png">
                                    <p align="left">
                                        Since we need to limit the output rot angles to be between -180 to 180 degrees, we use mapper.normalize_angle to normalize the angle before we output them. To ensure the correctness of my implementation, I wrote simple unit tests such as this

                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g4.png">
                                    <p align="left">
                                        TAs you can see, if a robot starts from point (0,0) while facing 0 degrees. It makes sense to turn 45 degrees, travel root 2 meters, and then turn -45 degrees.
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Odom_motion_model
                                    </p>
                                    <p align="left">
                                        This function takes three inputs: <code>cur_pose</code>, <code> prev_pose</code>, and <code>u</code>. It outputs the probability of traveling from one pose to another with the given control. I first calculated the <code>actual_u</code> by calling <code>compute_control(cur_pose, prev_pose)</code>. Then, I used three Gaussian functions with the actual control as means and noise as sigma. In this way, I found the probability. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g5.png">
                                    <p align="left">
                                        I can also unit test this helper function:
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g6.png">
                                    <p align="left">
                                        I inputted two poses and a u that is similar to the actual control and I got a high probability of 0.982, which is what I am expecting. 
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Prediction_step</p>
                                    <p align="left">
                                        Now, it is time to implement one of the two main steps of a Bayes filter. In this step, we update the probabilities in loc.bel_bar based on the previous loc.bel and the odometry motion model. Loc.bel_bar is an intermediate variable that is the output of the prediction step and we will use it in the update step very soon. Even though it is a global variable, we need to initialize it every time we call this function because it does not accumulate the previous bel_bar value. Before we entered the nested loops, we first computer u with the odometry data and initialize bel_bar. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g7.png">
                                    <p align="left">
                                        In the prediction step, we would like to calculate a new belief, bel_bar, given the previous belief and the odometry. On the first free outer loop, I looped through the X, Y, A of the previous belief. To speed up the code, I made a threshold value of 0.0001. I would only keep studying the possibility with this previous pose if the previous probability at the given XYA is higher than the threshold value. The uniform belief value was <code>1/(12*9*18) = 0.00051440</code>. Therefore, I picked the threshold to be the same order of magnitude. In the inner loops, I loop through all the possible X, Y, A values. For each one of them, I updated the bel_bar value with the odom_motion_model times previous bel. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g8.png">
                                    <p align="left">
                                        In the end, I normalized bel_bar to a sum of one. <code>loc.bel_bar = loc.bel_bar / np.sum(loc.bel_bar)</code>
                                    </p> 
                                    <p align="left">
                                        To test the prediction step, I can’t really make a unit test for it. When I am running it on the simulator, I initialized the probability distribution with a point mass distribution and can vaguely check if the number makes sense in the first two loops. This was the hardest part of debugging since there is no direct way to verify the correctness of the implementation. However, by unit testing the previous two helper functions, we are in a good place.
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Update_step
                                    </p>
                                    <p align="left">
                                        Lastly, in the update step, I updated the probabilities in loc.bel based on loc.bel_bar and the sensor model. At first, I tried to use matrix multiplication, but it was hard to debug and wrap my head around. Therefore, I used three nested loops to loop through X, Y, A in their respective range. For each initial pose, I looped through the 18 sensor measurements and used a gaussian with previous stored observations data at the pose as mean and sensor noise as sigma to find the probability of the robot at the pose with that sensor measurement. I used a “*=” operator to cascade the probability at the 18 independent sensor readings. 
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11g9.png">
                                    <p align="left">
                                        After we update the p (initialized to be 1 at each XYA), I update the bel value: <code>loc.bel[x][y][a] = loc.bel_bar[x][y][a] * p</code>.
                                        Similar to the prediction_step, I also normalize loc.bel before outputting it: <code>loc.bel = loc.bel / np.sum(loc.bel)</code>.                                         
                                    </p> 
                                    <p class="pre-wrap lead mb-3">Final Demo
                                    </p>
                                    <p align="left">
                                        After some intense and frustrating debugging, my final trajectory looks like this. Green is the ground truth. Red is the integrated odometry path. As you can see, it is pretty bad and that’s why we need Bayes filter. The blue line is the path from the center of the grid with the highest belief at each step. Since there is noise in the sensor measurement and the number of grids was not high enough, the path does not completely align with the ground truth path.                                
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab11p10.png">
                                    <p align="left">
                                        Real-time update of Ground truth, Odometry, Bel                            
                                    </p> 
                                    <iframe src = "https://www.youtube.com/embed/WUdDX8XwQ1c"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p align="left">
                                        Real-time update of Ground truth, Odometry, Bel Bel_bar                               
                                    </p>
                                    <iframe src = "https://www.youtube.com/embed/PWqYwe3oeZU"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="portfolio-modal modal fade" id="portfolioModal11" tabindex="-1" role="dialog" aria-labelledby="#portfolioModal11Label" aria-hidden="true">
            <div class="modal-dialog modal-xl" role="document">
                <div class="modal-content">
                    <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true"><i class="fas fa-times"></i></span></button>
                    <div class="modal-body text-center">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary mb-0">Lab 12: Localization (real)</h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <p class="pre-wrap lead mb-3">Objective </p>
                                    <p align="left">
                                        In this lab, we are starting to bring what we implemented in the previous lab into real-life implementation. We are starting with the update step of the Bayes filter, in which we simply spin in place and collect sensor data while comparing them to the pre-cached values. Implementing real-life means more noise will be involved and we will perform our spinning more precisely to reduce those noise.

                                    </p>  
                                    <p class="pre-wrap lead mb-3">Localization in Simulation
                                    </p>
                                    <p align="left">
                                        Before we moved to the real robot, we are verifying the simulation code again. This is exactly what we implemented last week, we are rerunning this time since we are using Vivek’s version now. As you can see, the <code>loc.bel</code> of the robot trajectory looks very similar to what I got last week. After verifying this, we can move on to the real robot.  
                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12sim.png">
                                    <p class="pre-wrap lead mb-3">Base Code Setup
                                    </p>
                                    <p align="left">
                                        Before implementing, I copied necessary files that enables bluetooth connetions from the previous lab. Those files include files “base_ble.py”, “ble.py”, “connection.yaml” and “cmd_types.py”. One can run the blocks with bluetooth conecction commands to test if base code is fully setup.
                                    </p>  
                                    <p class="pre-wrap lead mb-3">Perform_observation_loop implementation (Data collection)
                                    </p>
                                    <p align="left">
                                        “<code>def perform_observation_loop(self, rot_vel=120)</code>” performs the observation loop behavior on the real robot. This function lets the robot to turn 360 degree in place while collecting data at each 20 degree intervals. Since observations_count is defined to be 18 (=360/20), this function outputs <code>sensor_ranges</code>, a column numpy array of the range values (meters).
                                    </p>  
                                    <p align="left">
                                        In the beginning of the function I am sending commands to make the robot perform closed-loop turning for 20 degrees for 18 times. 

                                    </p>  
                                    <p align="left">
                                        First, I called: <code>ble.send_command(CMD.SETPIDVALUE, "0.5|0|0")</code> This command set the PID value. I used a different PID value than the drifting lab because I intentionally wants the robot to overshot and come back to overcome the friction on the ground. When the robot starts from rest, it needs to turn extra hard to overcome higher stall current. However, when it is already moving, it is easier to address to the correct angle by turning to the opposite direction. 
                                    </p>  
                                    <p align="left">
                                        Second, I called: <code>ble.send_command(CMD.SETSETPOINT, "0")</code> I also used this command to zero the setpoint. 
                                    </p>  
                                    <p align="left">
                                        Lastly, I called: <code>ble.send_command(CMD.MAP,"18")</code> I am reusing the “map” command from lab 10, in which my robot was performing closed loop turning in small intervals as well. It takes an input take tells the robot how many time it should turn. 
                                    </p>  
                                    <p align="left">
                                        On the Artemis side, the implementation of the PD controller is covered in lab6: 
                                    </p>  
                                    <img class="img-fluid" src="assets/img/Lab6:4.png">
                                    <p align="left">
                                        The reading distance process is the same as in the previous labs. We start ranging, wait until data is available, and store the read value into a variable, then stop ranging.
                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12c1.png">
                                    <p align="left">
                                        I implemented all mapping-related steps into the mapping commend. It takes the user input from the Bluetooth command for the number of data the robot collects. 
                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12c2.png">
                                    <p align="left">
                                        And then, I declared all the variables. Then, I am calculating the KP values after a new yaw value is ready. When the error is small enough (less than 0.2 degrees),  the robot brakes, toggles the led on Artemis and collects a TOF reading. When a reading is obtained, I increment the setpoint by 20 degrees until enough data point is collected. If the error is not small enough, the robot will turn to try to reach the setpoint. The TOF data are stored in an array that will be sent to the computer over bluetooth later. 

                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12c3.png">
                                    <p align="left">
                                        Back to the Python side, I ran a <code>time.sleep(13)</code> so the code pauses for 13 seconds, which is roughly the time the robot takes to spin for a full 360 degrees. 

                                    </p>  
                                    <iframe src = "https://www.youtube.com/embed/-Dd-lTT0Kpo"
                                    width="560" height="315" frameborder="0" allowfullscreen></iframe>
                                    <p class="pre-wrap lead mb-3">Perform_observation_loop implementation (Data Transmission)
                                    </p>
                                    <p align="left">
                                        After we collected the data on the Artemis side, we now need to send them over. I first printed out "<code>start sending data</code>" for debugging purposes. Then I declared an empty array <code>readings =[]</code>.
                                        Then we used the handler like how we used it since lab2. 
                                        
                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12c4.png">
                                    <p align="left">
                                        We used it in multiple labs by now, so it should work reliable. However, there is a twist. In the previous lab, we can run this in its own block so that we can wait for it to run and check the updating readings array later. 

                                    </p>  
                                    <p align="left">
                                        For this function, I used the code below to format the data nicely into the the 18 rows numpy arraries that I declared as output. Units are very important, I forgot to convert the mm readings into cm and that will result in a wrong prediction. 

                                    </p>  
                                    <img class="img-fluid" src="assets/img/lab12c5.png">
                                    <p align="left">
                                        I also bumped into a more serious problem. The Artemis needs time to send data over but the Python code is executed sequentially so when we are trying to access readings, we will get an out of bounds error. One may naturelly thinks that we can simply add a time.sleep function like we waited for the robot to spin. This time it is a bit different though. The handler needs to be triggered so that the Python code and receive the sent data. When the code is sleeping, nothing will happen. 
                                    </p>  

                                    <p align="left">
                                        To solve this problem, I use the <code>await asyncio.sleep(2)</code> function to make the Python program idle but not blocked by anything. When we call the await function, it will ask the event loop to run something else while your await statement finishes its execution. In order to use the await asyncio.sleep function, we need to add the async keyword to the function defintion <code>RealRobot.perform_observation_loop()</code>, <code>BaseLocalization.get_observation_data()</code> and the await keyword to <code>perform_observation_loop()</code> and <code>loc.get_observation_data()</code>. With this fix, my code can be executed without errors. 
                                    </p>  
                                    <p class="pre-wrap lead mb-3">Demo & Result
                                    </p>
                                    <p align="left">
                                        To demo, we simply just run the block that runs an update step of the Bayes filter. 
                                        
                                    </p>  
                                    <p align="left">
                                        1. The first point I spun my robot at was (-3 ft ,-2 ft ,0 deg), or (-0.914m, 0.610m, 0 deg). 
                                        
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab12p1.png">
                                    <p align="left">
                                        The belief I got from the update step is (-0.914m, -0.610m, 30deg). The x and y are exactly the same and the angle is a bit off, but at a reasonable range of error. The surrounding of this position is quite asymmetric, and therefore, obtaining a good belif is easier.
                                        
                                    </p> 
                                    <p align="left">
                                        2. The second point is (5 ft,3 ft,0 deg),（1.524m, 0.9144m, 0deg)
                                        
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab12p2.png">
                                    <p align="left">
                                        The belief I got from the update step is (-0.914m, 0.610m, -10deg). Both x and y are one index off and the angle is only 10 degree off. The belief is still relatively accurate. The source of error maybe that the data point from the square obstacle is scanned at a very sharp angle, which means when if the robot does not turn to the exact angle, the those 1-2 data point maybe very wrong. 
                                        
                                    </p> 
                                    <p align="left">
                                       3. The third point is (5 ft,-3 ft,0 deg), or (1.524m, -0.9144m, 0deg). 
                                        
                                    </p> 
                                    <img class="img-fluid" src="assets/img/lab12p3.png">
                                    <p align="left">
                                        The belief I got from the update step is (1.524m, -0.9144m, -10deg). This point did really well  because it position is pretty unique and it does not have any surface at a sharp angle. The surrounding of this position is also quite asymmetric, and therefore, obtaining a good belif is easier.
                                    </p> 
                                    <p align="left">
                                        4. The last point is (0 ft,3 ft,0 deg), or (0 m,0.914m,0 deg)
                                         
                                     </p> 
                                     <img class="img-fluid" src="assets/img/lab12p4.png">

                                     <p align="left">
                                        The belief I got from the update step is (0 m,0.914m, -10deg). This point seems to be accurate but I got it after a couple of trails. This is the most challenging point because the positive x and negative y directions from this point are all quite far. Especially in negative y direction, the robot would miss that obstacle if it did not face perpendicular down at some point and collected that data point. This position is very unique, yet the robot needs to spin perfectly to capture all the unique data points. For this point, I also try to increase <code>sensor_sigma</code> value. It has more accurate convegences with a higher <code>sensor_sigma</code>. 
                                         
                                     </p> 
                                    
                                    
                                    <button class="btn btn-primary" href="#" data-dismiss="modal"><i class="fas fa-times fa-fw"></i>Close Window</button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <section class="page-section bg-primary text-white mb-0" id="about">
            <div class="container">
                <!-- About Section Heading-->
                <div class="text-center">
                    <h2 class="page-section-heading d-inline-block text-white">ABOUT ME</h2>
                </div>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- About Section Content-->
                <div class="row">
                    <div class="col-lg-6 ">
                        <p class="pre-wrap lead mb-3">Hi, I am Robby, an ECE undergrad minor in Robotics.   I am an interdisciplinary engineer. <p class="pre-wrap lead">I lead the Electrical team of Cornell Electric Vehicles project team and develop double tail single actuated SAW robots in CEI Lab. I play soccer, ping pong, guitar, and do photography in my spare time. </p>
                    </div>
                    <div class="col ">
                        </div><img class="img-fluid" src="assets/img/headshot.jpg" width="410" height="150">
                    </div>
                    <div class="col mr-auto">
                        <p class="mb-5">  </p>
                    </div>
                </div>
            </div>
        </section>
        <section class="page-section" id="contact">
            <div class="container">
                <!-- Contact Section Heading-->
                <div class="text-center">
                    <h2 class="page-section-heading text-secondary d-inline-block mb-0">CONTACT</h2>
                </div>
                <!-- Icon Divider-->
                <div class="divider-custom">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Contact Section Content-->
                <div class="row justify-content-center">
                    <div class="col-lg-4">
                        <div class="d-flex flex-column align-items-center">
                            <div class="icon-contact mb-3"><i class="fas fa-mobile-alt"></i></div>
                            <div class="text-muted">Phone</div>
                            <div class="lead font-weight-bold">(781) 417-9771</div>
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="d-flex flex-column align-items-center">
                            <div class="icon-contact mb-3"><i class="far fa-envelope"></i></div>
                            <div class="text-muted">Email</div><a class="lead font-weight-bold">lh479@cornell.edu</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- <footer class="footer text-center">
            <div class="container">
                <div class="row"> -->
                    <!-- Footer Location-->
                    <!-- <div class="col-lg-4 mb-5 mb-lg-0">
                        <h4 class="mb-4">LOCATION</h4>
                        <p class="pre-wrap lead mb-0">2215 John Daniel Drive
Clark, MO 65243</p>
                    </div> -->
                    <!-- Footer Social Icons-->
                    <!-- <div class="col-lg-4 mb-5 mb-lg-0">
                        <h4 class="mb-4">AROUND THE WEB</h4><a class="btn btn-outline-light btn-social mx-1" href="https://www.facebook.com/StartBootstrap"><i class="fab fa-fw fa-facebook-f"></i></a><a class="btn btn-outline-light btn-social mx-1" href="https://www.twitter.com/sbootstrap"><i class="fab fa-fw fa-twitter"></i></a><a class="btn btn-outline-light btn-social mx-1" href="https://www.linkedin.com/in/startbootstrap"><i class="fab fa-fw fa-linkedin-in"></i></a><a class="btn btn-outline-light btn-social mx-1" href="https://www.dribble.com/startbootstrap"><i class="fab fa-fw fa-dribbble"></i></a>
                    </div>-->
                    <!-- Footer About Text-->
                    <!-- <div class="col-lg-4">
                        <h4 class="mb-0">ABOUT FREELANCER</h4>
                        <p class="pre-wrap lead mb-0">Freelance is a free to use, MIT licensed Bootstrap theme created by Start Bootstrap</p>
                    </div>
                </div>
            </div>
        </footer> -->
        <!-- Copyright Section-->
        <section class="copyright py-4 text-center text-white">
            <div class="container"><small class="pre-wrap">Copyright © Robby Huang 2022</small></div>
        </section>
        <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes)-->
        <div class="scroll-to-top d-lg-none position-fixed"><a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top"><i class="fa fa-chevron-up"></i></a></div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>